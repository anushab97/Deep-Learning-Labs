{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPeQ7s/wFZ6+zqdhp8/ePuN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Part 2 of Lab 01:**"],"metadata":{"id":"PRsLKDBiYx-a"}},{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv('anemia.csv')\n","len_data = len(data)\n","print(\"Total instances in original training dataset:\", len_data)\n","X = data.iloc[ : , 0:5]\n","y = data.iloc[ : , 5:6]"],"metadata":{"id":"IDog7-bo8n-5","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662999176688,"user_tz":360,"elapsed":681,"user":{"displayName":"Anusha Basavaraja","userId":"15596356178355955222"}},"outputId":"01d65379-5888-4225-99ac-000317f53500"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Total instances in original training dataset: 1421\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gHlajt8B6HJL","executionInfo":{"status":"ok","timestamp":1662999179160,"user_tz":360,"elapsed":624,"user":{"displayName":"Anusha Basavaraja","userId":"15596356178355955222"}}},"outputs":[],"source":["# Create the train/test split and preprocess\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n","                                                    random_state=1)\n","\n","# Standardize data\n","from sklearn.preprocessing import StandardScaler\n","stdsc = StandardScaler()\n","stdsc.fit(X_train)\n","X_train_std = stdsc.transform(X_train)\n","X_test_std = stdsc.transform(X_test)"]},{"cell_type":"code","source":["# Considering only 20%, 40%, 60%, 80% and 100% from original 70% of \n","#training data for experimenting with the models.\n","\n","X_train_std_20 = X_train_std[0 : int(0.20 * len(X_train_std))]\n","y_train_20 = y_train[0 : int(0.20 * len(X_train_std))]\n","\n","X_train_std_40 = X_train_std[0 : int(0.40 * len(X_train_std))]\n","y_train_40 = y_train[0 : int(0.40 * len(X_train_std))]\n","\n","X_train_std_60 = X_train_std[0 : int(0.60 * len(X_train_std))]\n","y_train_60 = y_train[0 : int(0.60 * len(X_train_std))]\n","\n","X_train_std_80 = X_train_std[0 : int(0.80 * len(X_train_std))]\n","y_train_80 = y_train[0 : int(0.80 * len(X_train_std))]"],"metadata":{"id":"KeSq8VSw9IbL","executionInfo":{"status":"ok","timestamp":1662999181785,"user_tz":360,"elapsed":255,"user":{"displayName":"Anusha Basavaraja","userId":"15596356178355955222"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Prining out the length of training data sizes\n","'''\n","print(\"len_20% of training: {} and test: {}\". format(len(X_train_std_20), len(y_train_20)))\n","print(\"len_40% of training: {} and test: {}\". format(len(X_train_std_40), len(y_train_40)))\n","print(\"len_60% of training: {} and test: {}\". format(len(X_train_std_60), len(y_train_60)))\n","print(\"len_80% of training: {} and test: {}\". format(len(X_train_std_80), len(y_train_80)))\n","print(\"len_100% of training: {} and test: {}\". format(len(X_train_std), len(y_train)))\n","'''"],"metadata":{"id":"CrLqW4XJ_AvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import all packages required\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import metrics\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"evpgR-_2_0X9","executionInfo":{"status":"ok","timestamp":1662999200820,"user_tz":360,"elapsed":272,"user":{"displayName":"Anusha Basavaraja","userId":"15596356178355955222"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["n_start_itr = 5\n","n_end_itr = 200\n","n_itr_inc = 5\n","\n","def train_size_expt(X_train_data, y_train_data, size_training_data):\n","  epoch_counts = []\n","  accuracy_observations = []\n","  for epochs in range(n_start_itr, n_end_itr, n_itr_inc):\n","    mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=(10, 25, 50), \n","                        max_iter=epochs, random_state=1)\n","    mlp.fit(X_train_data, y_train_data)\n","    accuracy_score = mlp.score(X_train_data, y_train_data)\n","    accuracy_observations.append(accuracy_score)\n","    epoch_counts.append(epochs)\n","\n","  return accuracy_observations, epoch_counts, size_training_data"],"metadata":{"id":"YeFrzm-ZCFEo","executionInfo":{"status":"ok","timestamp":1663002439482,"user_tz":360,"elapsed":136,"user":{"displayName":"Anusha Basavaraja","userId":"15596356178355955222"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["result = []\n","epoch_steps = []\n","\n","# Using 20% of original 70% training data \n","result_20 = train_size_expt(X_train_std_20, y_train_20, 20)\n","result.append(result_20[0])\n","epoch_steps.append(result_20[1])\n","\n","# Using 40% of original 70% training data \n","result_40 = train_size_expt(X_train_std_40, y_train_40, 40)\n","result.append(result_40[0])\n","epoch_steps.append(result_40[1])\n","\n","# Using 60% of original 70% training data \n","result_60 = train_size_expt(X_train_std_60, y_train_60, 60)\n","result.append(result_60[0])\n","epoch_steps.append(result_60[1])\n","\n","# Using 80% of original 70% training data \n","result_80 = train_size_expt(X_train_std_80, y_train_80, 80)\n","result.append(result_80[0])\n","epoch_steps.append(result_80[1])\n","\n","# Using 100% of original 70% training data \n","result_100 = train_size_expt(X_train_std, y_train, 100)\n","result.append(result_100[0])\n","epoch_steps.append(result_100[1])\n","\n","plt.xlabel(\"Num of epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Impact of number of training epochs\")\n","for i in range(0, len(result)):\n","  plt.plot(epoch_steps[i], result[i], linewidth = 2)\n","\n","plt.show()"],"metadata":{"id":"Yx62ln6Rch2X"},"execution_count":null,"outputs":[]}]}